import gradio as gr
import torch
import torch.nn.functional as F
import cv2
import numpy as np
import os
from src.preprocess_image import preprocess_document_image
from ultralytics import YOLO
from torchvision import transforms
from my_mobilenet import MyMobileNetV2
import easyocr

# ========== 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π ==========

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
cls_model = MyMobileNetV2(num_classes=5, pretrained=False)
state_dict = torch.load("./gradio/models/best_doc_classifier.pth", map_location=device)
cls_model.load_state_dict(state_dict)
cls_model.to(device)
cls_model.eval()

class_labels = ["attestat", "diplom", "passport", "prilozenia", "snils"]

# YOLO-–º–æ–¥–µ–ª–∏
yolo_models = {
    "attestat": YOLO("./gradio/models/attestat.pt"),
    "diplom": YOLO("./gradio/models/diplom.pt"),
    "passport": YOLO("./gradio/models/passport.pt"),
    "snils": YOLO("./gradio/models/snils_model.pt"),
    "prilozenia": YOLO("./gradio/models/attestat.pt")
}

# EasyOCR (—Ç–≤–æ—è –∫–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å)
MODEL_DIR_PATH = "/mnt/mishutqa/PycharmProjects/sirius/gradio/custom_EasyOCR"
reader = easyocr.Reader(
    ['ru'],
    model_storage_directory=f"{MODEL_DIR_PATH}/model",
    user_network_directory=f"{MODEL_DIR_PATH}/user_network",
    recog_network='custom_example',
    # detector=False,
    gpu=True,
    download_enabled=False
)

# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
cls_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# ========== 2. –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ==========

def classify_image(image):
    img_tensor = cls_transform(image).unsqueeze(0).to(device)
    with torch.no_grad():
        outputs = cls_model(img_tensor)
        probs = F.softmax(outputs, dim=1)
        pred_idx = probs.argmax(dim=1).item()
        return class_labels[pred_idx]


def crop_by_obb(image, obb, doc_label="document", zone_name="zone", output_dir_base="output"):
    """
    –í—ã—Ä–µ–∑–∞–µ—Ç —Ä–µ–≥–∏–æ–Ω –ø–æ OBB, –ø–æ–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç, –µ—Å–ª–∏ –æ–Ω –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
    """
    pts = np.array(obb, dtype=np.float32).reshape(4, 2)

    # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º —Ç–æ—á–∫–∏: tl, tr, br, bl
    rect = np.zeros((4, 2), dtype="float32")
    s = pts.sum(axis=1)
    rect[0] = pts[np.argmin(s)]  # tl
    rect[2] = pts[np.argmax(s)]  # br
    diff = np.diff(pts, axis=1)
    rect[1] = pts[np.argmin(diff)]  # tr
    rect[3] = pts[np.argmax(diff)]  # bl

    (tl, tr, br, bl) = rect
    width = max(int(np.linalg.norm(br - bl)), int(np.linalg.norm(tr - tl)))
    height = max(int(np.linalg.norm(tr - br)), int(np.linalg.norm(tl - bl)))

    if width <= 0 or height <= 0:
        return None

    dst = np.array([
        [0, 0],
        [width - 1, 0],
        [width - 1, height - 1],
        [0, height - 1]
    ], dtype="float32")

    M = cv2.getPerspectiveTransform(rect, dst)
    warped = cv2.warpPerspective(image, M, (width, height))

    # === –ü–û–í–û–†–û–¢ –î–õ–Ø –í–ï–†–¢–ò–ö–ê–õ–¨–ù–´–• –§–†–ê–ì–ú–ï–ù–¢–û–í ===
    if warped is not None and warped.size > 0:
        h, w = warped.shape[:2]
        if h > w:  # –≤—ã—Å–æ—Ç–∞ > —à–∏—Ä–∏–Ω—ã ‚Üí –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
            warped = cv2.rotate(warped, cv2.ROTATE_90_COUNTERCLOCKWISE)

    # === –°–û–•–†–ê–ù–ï–ù–ò–ï –í –§–ê–ô–õ ===
    output_subdir = os.path.join(output_dir_base, doc_label)
    os.makedirs(output_subdir, exist_ok=True)

    existing_files = [f for f in os.listdir(output_subdir) if f.startswith(zone_name)]
    next_idx = len(existing_files)
    save_path = os.path.join(output_subdir, f"{zone_name}_{next_idx:03d}.jpg")
    cv2.imwrite(save_path, warped)

    return warped


def yolo_ocr_to_json(cls_label, yolo_results, image):
    zones_json = []

    for det in yolo_results[0].obb:
        obb = det.xyxyxyxy.cpu().numpy().flatten().tolist()
        zone_id = int(det.cls)
        zone_name = yolo_models[cls_label].names[zone_id]

        cropped = crop_by_obb(image, obb, cls_label)
        if cropped is None or cropped.size == 0:
            continue

        # –î–ª—è stamp/gerb ‚Äî –Ω–µ –Ω—É–∂–Ω–æ OCR
        if zone_name in ('stamp', 'gerb'):
            zones_json.append({zone_name: True})
            continue

        ocr_result = reader.readtext(cropped)

        if not ocr_result:
            continue

        # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–º —Å—Ç—Ä–æ–∫–∞–º –≤ –∑–æ–Ω–µ
        pred_text = " ".join([text for (_, text, _) in ocr_result]).strip()

        # print("\n".join([text for (_, text, _) in ocr_result]).strip(), angle, mean_conf)
        # –ï—Å–ª–∏ –Ω–∏ –≤ –æ–¥–Ω–æ–π –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É

        zones_json.append({zone_name: pred_text})

    return {
        "document": {
            "doc_name": cls_label,
            "zones": zones_json
        }
    }


# ========== 3. –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω ==========
def process_image(image):
    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

    # 1. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    cls_label = classify_image(image)

    rotated_image = preprocess_document_image(image_bgr)

    # 2. YOLO OBB
    yolo_model = yolo_models.get(cls_label)
    results = yolo_model(rotated_image)
    print(results[0].obb.conf)
    print(torch.mean(results[0].obb.conf))


    # 3. OCR + JSON
    ocr_results = yolo_ocr_to_json(cls_label, results, rotated_image)

    return ocr_results

# ========== 4. Gradio –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å ==========


iface = gr.Interface(
    fn=process_image,
    inputs=gr.Image(type="numpy", label="–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"),
    outputs=gr.JSON(label="–†–µ–∑—É–ª—å—Ç–∞—Ç"),
    title="üß† OCR-—Å–µ—Ä–≤–∏—Å —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –∏ OBB",
    description="MobileNet ‚Üí YOLOv8 OBB ‚Üí EasyOCR (–∫–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å)"
)

if __name__ == "__main__":
    iface.launch(server_name="0.0.0.0", server_port=7861)